{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Kaggle: How Much Did It Rain\n",
      "URL: https://www.kaggle.com/c/how-much-did-it-rain"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%load_ext cythonmagic"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np\n",
      "import collections\n",
      "from sklearn.preprocessing import StandardScaler\n",
      "from sklearn.decomposition import PCA\n",
      "from sklearn.svm import SVC\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "from sklearn.pipeline import Pipeline\n",
      "\n",
      "from sklearn.feature_selection import SelectKBest\n",
      "from sklearn.feature_selection import f_regression\n",
      "from sklearn.grid_search import GridSearchCV\n",
      "from sklearn.metrics import make_scorer\n",
      "\n",
      "from sklearn.base import BaseEstimator, TransformerMixin\n",
      "from sklearn.base import BaseEstimator, TransformerMixin\n",
      "from sklearn.utils import warn_if_not_float\n",
      "from sklearn.utils import check_arrays\n",
      "#from sklearn.utils.validation import check_is_fitted\n",
      "from sklearn.preprocessing.data import _mean_and_std\n",
      "from itertools import *\n",
      "\n",
      "import six\n",
      "import csv\n",
      "\n",
      "from IPython.display import clear_output\n",
      "import sys\n",
      "import os\n",
      "import time"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 35
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "np.set_printoptions(precision=3)\n",
      "np.set_printoptions(suppress=True)\n",
      "np.set_printoptions(linewidth=130)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 36
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "zip = six.moves.zip\n",
      "map = six.moves.map\n",
      "range = six.moves.range"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 37
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Helpful functions\n",
      "Includes __Vectorizers__ used to vectorize data."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%cython\n",
      "cimport numpy as np\n",
      "import numpy as np\n",
      "import collections\n",
      "from itertools import *\n",
      "import os\n",
      "\n",
      "class Vectorizer(object):\n",
      "\n",
      "    \"\"\"Turn a matrix into a vector\n",
      "    \"\"\"\n",
      "    @staticmethod\n",
      "    def get_all():\n",
      "        return Vectorizer.__subclasses__()\n",
      "\n",
      "class IntegrationMethod(Vectorizer):\n",
      "\n",
      "    \"\"\"Produce a vector which is the trapizoidally integrated quanties.\n",
      "    \"\"\"\n",
      "    @staticmethod\n",
      "    def vectorize(mat):\n",
      "        \"\"\"\n",
      "        \"\"\"\n",
      "        a = np.zeros(mat.shape[0]-1, dtype=np.float)\n",
      "        a += mat[1:,0] * (60 - mat[0, 0]) + mat[1:,-1] * (mat[0, -1])\n",
      "        for i in xrange(mat.shape[1]-1):\n",
      "            a += 0.5 * (mat[0, i] - mat[0, i+1]) * (mat[1:, i] + mat[1:, i+1])\n",
      "        return a\n",
      "\n",
      "class MeanMethod(Vectorizer):\n",
      "\n",
      "    \"\"\"Produce a vector which is the mean of quanties.\n",
      "    \"\"\"\n",
      "    @staticmethod\n",
      "    def vectorize(mat):\n",
      "        \"\"\"\n",
      "        \"\"\"\n",
      "        return np.mean(mat[1:, :], axis=1)\n",
      "\n",
      "class DifferenceMethod(Vectorizer):\n",
      "\n",
      "    \"\"\"Produce a vector which is the difference of start and stop quantities.\n",
      "    \"\"\"\n",
      "    @staticmethod\n",
      "    def vectorize(mat):\n",
      "        \"\"\"\n",
      "        \"\"\"\n",
      "        return mat[1:, -1] - mat[1:, 0]\n",
      "\n",
      "\n",
      "def accumulate(class_weights, size=70):\n",
      "    \"\"\"Produce a CDF from weighted classes.\n",
      "    \"\"\"\n",
      "    cdf = np.zeros(size)\n",
      "    for i, j in class_weights:\n",
      "        if i >= size:\n",
      "            break\n",
      "        cdf[i] += j\n",
      "    for i in xrange(1, cdf.shape[0]):\n",
      "        cdf[i] += cdf[i-1]\n",
      "    return cdf\n",
      "\n",
      "heaviside = lambda x: 1 if x >= 0 else 0\n",
      "\n",
      "def consume(iterator, n):\n",
      "    \"Advance the iterator n-steps ahead. If n is none, consume entirely.\"\n",
      "    # Use functions that consume iterators at C speed.\n",
      "    if n is None:\n",
      "        # feed the entire iterator into a zero-length deque\n",
      "        collections.deque(iterator, maxlen=0)\n",
      "    else:\n",
      "        # advance to the empty slice starting at position n\n",
      "        next(islice(iterator, n, n), None)\n",
      "\n",
      "def grouper(iterable, n, fillvalue=None):\n",
      "    \"Collect data into fixed-length chunks or blocks\"\n",
      "    # grouper('ABCDEFG', 3, 'x') --> ABC DEF Gxx\n",
      "    args = [iter(iterable)] * n\n",
      "    return izip_longest(fillvalue=fillvalue, *args)\n",
      "\n",
      "def batch_read(file_name, batch_size=1000, skip_lines=0):\n",
      "    \"\"\"Batch the reads of a file\n",
      "    \"\"\"\n",
      "    with open(file_name, 'r') as fh:\n",
      "        rls = iter(fh.readlines())\n",
      "        consume(rls, skip_lines)\n",
      "        for lst in iter(grouper(rls, batch_size)):\n",
      "            yield lst\n",
      "\n",
      "def file_headers(file_name, sep=','):\n",
      "    \"\"\"Pluck the header from a file and output into a list.\n",
      "    \"\"\"\n",
      "    with open(file_name, 'r') as fh:\n",
      "        return fh.readline().rstrip().split(sep)\n",
      "    \n",
      "    \n",
      "def tail(file_name, lines=10):\n",
      "    \"\"\"\n",
      "    \"\"\"\n",
      "    with open(file_name, 'rb') as fh:\n",
      "        fh.seek(-2, os.SEEK_END)\n",
      "        for i in xrange(lines):\n",
      "            while fh.read(1) != \"\\n\":\n",
      "                fh.seek(-2, os.SEEK_CUR)\n",
      "        return fh.readline()\n",
      "    \n",
      "def parseline(text, columns):\n",
      "    \"\"\"Parse data line and turn it into a matrix.\n",
      "    \"\"\"\n",
      "    nanzero = lambda x: x if not np.isnan(x) else 0\n",
      "    if text is None:\n",
      "        return None\n",
      "    text = text.rstrip()\n",
      "    index = 0\n",
      "    expected = 0\n",
      "    data_cols = []\n",
      "    i = 0\n",
      "    for i, col in enumerate(text.split(',')):\n",
      "        if columns[i] == 'Id':\n",
      "            index = int(col)\n",
      "        elif columns[i] == 'Expected':\n",
      "            expected = float(col)\n",
      "        elif columns[i] == 'LogWaterVolume':\n",
      "            data_cols.append([nanzero(np.exp(float(x))) for x in col.split(' ')])\n",
      "        else:\n",
      "            data_cols.append([nanzero(float(x)) for x in col.split(' ')])\n",
      "    return (index, np.array(data_cols, dtype=float), expected)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 41
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "class IncrementalStandardScaler(BaseEstimator, TransformerMixin):\n",
      "\n",
      "    def __init__(self):\n",
      "        self.n_ = 0\n",
      "\n",
      "    def partial_fit(self, X, y=None):\n",
      "        \"\"\"Compute the mean and std to be used later.\n",
      "        :X: array-like or CSR matrix with shape [n_samples, n_features]\n",
      "        \"\"\"\n",
      "        if 'mean_' in dir(self):\n",
      "            X = X.astype(np.float)\n",
      "            n1 = self.n_\n",
      "            m1 = self.mean_\n",
      "            v1 = self.var_\n",
      "            n2 = len(X)\n",
      "            m2 = X.mean(axis=0)\n",
      "            v2 = X.var(axis=0)\n",
      "            m = (n1 * m1 + n2 * m2) / (n1 + n2)\n",
      "            self.var_ = (n1 * (m1 - m) ** 2 + n2 * (m2 - m) ** 2 + v1 * (n1 -1) + v2 * (n2 - 1)) / (n1 + n2 - 1)\n",
      "            self.mean_ = m\n",
      "            self.n_ += n2\n",
      "        else:\n",
      "            self.mean_ = X.mean(axis=0)\n",
      "            self.var_ = X.var(axis=0)\n",
      "        \n",
      "    def fit(self, X, y=None):\n",
      "        \"\"\"Compute the mean and std to be used for later scaling.\n",
      "        Parameters\n",
      "        ----------\n",
      "        X : array-like or CSR matrix with shape [n_samples, n_features]\n",
      "            The data used to compute the mean and standard deviation\n",
      "            used for later scaling along the features axis.\n",
      "        \"\"\"\n",
      "        del self.mean_\n",
      "        del self.var_\n",
      "        self.n_ = 0\n",
      "        self.partial_fit(X)\n",
      "\n",
      "    def transform(self, X, y=None):\n",
      "        \"\"\"Perform standardization by centering and scaling\n",
      "        Parameters\n",
      "        ----------\n",
      "        X : array-like with shape [n_samples, n_features]\n",
      "            The data used to scale along the features axis.\n",
      "        \"\"\"\n",
      "        if 'mean_' not in dir(self):\n",
      "            print \"Not fitted!\"\n",
      "            return None\n",
      "\n",
      "        X = X.astype(np.float)\n",
      "        X -= self.mean_\n",
      "        X /= self.var_ * self.var_\n",
      "        return X\n",
      "\n",
      "    def inverse_transform(self, X):\n",
      "        \"\"\"Scale back the data to the original representation\n",
      "        Parameters\n",
      "        ----------\n",
      "        X : array-like with shape [n_samples, n_features]\n",
      "            The data used to scale along the features axis.\n",
      "        \"\"\"\n",
      "        X = X.copy()\n",
      "        X *= self.std_\n",
      "        X += self.var_ * self.var_\n",
      "        return X"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 39
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "scaler = IncrementalStandardScaler()\n",
      "\n",
      "batch_size = 10000\n",
      "data_file = 'data/train_2013_small.csv'\n",
      "ndatapoints = int(tail(data_file, 1).split(',')[0])\n",
      "headers = file_headers(data_f\n",
      "                       )\n",
      "t_start = time.time()\n",
      "for batch_n, ents in enumerate(batch_read('data/train_2013.csv', batch_size, 1)):\n",
      "    \n",
      "    Xs = []\n",
      "    y = []\n",
      "    clear_output()\n",
      "    print 'Processing batch {:5d} {:4.1f}% complete {:5.1f} p/s'.format(batch_n, 100.0 * batch_size * (batch_n + 1) / ndatapoints, float(batch_size) / (time.time() - t_start) )\n",
      "    sys.stdout.flush()\n",
      "    t_start = time.time()\n",
      "    for ent in ents:\n",
      "        batch_count += 1\n",
      "        if ent is None:\n",
      "            break\n",
      "        i, a, b = parseline(ent, headers)\n",
      "        y.append(b)\n",
      "        Xs.append(np.concatenate([vectorizer.vectorize(a) for vectorizer in Vectorizer.get_all()]))\n",
      "    scaler.partial_fit(np.array(Xs))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Processing batch    70 63.0% complete 1867.0 p/s\n"
       ]
      },
      {
       "ename": "KeyboardInterrupt",
       "evalue": "",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
        "\u001b[1;32m<ipython-input-49-b5910f02216f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     17\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0ment\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m             \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m         \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparseline\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ment\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m         \u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m         \u001b[0mXs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mvectorizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvectorize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mvectorizer\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mVectorizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_all\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
       ]
      }
     ],
     "prompt_number": 49
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Read in data"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "raw_data = []\n",
      "data_fh = open('data/train_2013_medium.csv', 'r')\n",
      "columns = data_fh.readline().rstrip().split(',')\n",
      "\n",
      "for i, line in enumerate(data_fh.readlines()):\n",
      "    if i % 100000 == 0:\n",
      "        print \"Read {:6d} measurments\".format(i)\n",
      "    raw_data.append(parseline(line, columns))\n",
      "    \n",
      "print \"Data points: \", len(raw_data)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Read      0 measurments\n",
        "Read 100000 measurments"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Read 200000 measurments"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Read 300000 measurments"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Read 400000 measurments"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Read 500000 measurments"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Read 600000 measurments"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Read 700000 measurments"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Read 800000 measurments"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Read 900000 measurments"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Data points: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 1000000\n"
       ]
      }
     ],
     "prompt_number": 20
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Vectorize Data"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "X = np.array([np.concatenate([vectorizer.vectorize(hs[1]) for vectorizer in Vectorizer.get_all()]) \n",
      "              for hs in raw_data])\n",
      "\n",
      "Y = np.array([int(np.ceil(x[2])) for x in raw_data])\n",
      "\n",
      "#del raw_data"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 21
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Scorer\n",
      "\n",
      "$$\n",
      "score = \\sum_N \\sum_{n=1}^{70} ( (P(y \\le n)) - H(n - z))^2\n",
      "$$"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def crps(est, X, y, **kwargs):\n",
      "    \"\"\"Continuous Ranked Probability Score\n",
      "    \"\"\"\n",
      "    classes = est.named_steps['clf'].classes_\n",
      "    \n",
      "    cdfs = [accumulate(zip(classes, z)) for z in est.predict_proba(X)]\n",
      "    N = len(cdfs)\n",
      "    M = len(cdfs[0])\n",
      "    tot = 0\n",
      "    for i, cdf in enumerate(cdfs):\n",
      "        for n in xrange(M):\n",
      "            tot += (cdf[n] - heaviside(n - y[i])) ** 2\n",
      "    return -float(tot) / (M * N)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 22
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Define pipeline, CV Parameters and run grid search fit"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "pipeline = Pipeline([('scaler', StandardScaler()), \n",
      "          ('decomp', PCA(n_components=0.9)),\n",
      "          #('anova', SelectKBest(f_regression)),\n",
      "          ('clf', SVC(probability=True))])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 23
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "parameters = {\n",
      "              'decomp__n_components': [0.99],\n",
      "              #'anova__k': [5, 10],\n",
      "              'clf__C': [1, 3, 10, 30],\n",
      "              #'clf__penalty': ['l1', 'l2'],\n",
      "              'clf__gamma': [0, 0.1, 0.3, 1.0 ],\n",
      "              'clf__degree': [ 3, 4, 5],\n",
      "              }\n",
      "clf = GridSearchCV(pipeline, parameters, verbose=1, n_jobs=2, scoring=crps)\n",
      "clf.fit(X, Y)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Fitting 3 folds for each of 48 candidates, totalling 144 fits\n"
       ]
      }
     ],
     "prompt_number": "*"
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "for key, val in  clf.best_params_.iteritems():\n",
      "    print \"{:20} => {}\".format(key, val)\n",
      "print \"Best Score =\", clf.best_score_"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": "*"
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Process Testing Data and write to CSV"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def vectorize_data(dat):\n",
      "    return np.concatenate([vectorizer.vectorize(dat) for vectorizer in Vectorizer.get_all()])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": "*"
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "test_data_fh = open('data/test_2014.csv', 'r')\n",
      "test_results_fh = open('data/test_2014_results.csv', 'w')\n",
      "\n",
      "writer = csv.writer(test_results_fh, delimiter=',')\n",
      "test_columns = test_data_fh.readline().rstrip().split(',')\n",
      "writer.writerow(['Id'] + ['Predicted{0}'.format(t) for t in xrange(0, 70)])\n",
      "\n",
      "\n",
      "classes = np.unique(clf.predict(X))\n",
      "\n",
      "for line in test_data_fh.readlines():\n",
      "    i, dat, _ =  parseline(line, test_columns)\n",
      "    cdf = accumulate(zip(classes, clf.predict_proba(vectorize_data(dat))[0]))\n",
      "    writer.writerow([i] + list(cdf))\n",
      "test_data_fh.close()\n",
      "test_results_fh.close()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": "*"
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "raw_data[0][1]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": "*"
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}